{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b12075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "from unicodedata import category\n",
    "import pandas as pd\n",
    "punctuation_chars_numbers =  set([chr(i) for i in range(sys.maxunicode) \n",
    "                             if category(chr(i)).startswith(\"P\") or category(chr(i)).startswith(\"Nd\")]) # Добавили кучу пунктуационных знаков и чисел\n",
    "print(punctuation_chars_numbers)\n",
    "\n",
    "sw = set(stopwords.words('russian'))\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem(entire_input=False)\n",
    "\n",
    "def MyTokenizer(sentence):\n",
    "    tokens = mystem_analyzer.lemmatize(sentence.lower())\n",
    "    tokens = [x for x in tokens if (len(set(x) & punctuation_chars_numbers) == 0 and len(set([x]) & sw) == 0 and len(x)>2)]\n",
    "    return ' '.join(tokens)\n",
    "# Использую mystem для лемматицазиции и удаляю большой список лишних символов и слов из текста с помощью пересечения(&) сетов.\n",
    "# Пересечение гораздо быстрее работает, чем поэлементная проверка с помощью if x in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a760280",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/seara/Desktop/Colab Notebooks/homework/hw06-text/train.csv')\n",
    "test = pd.read_csv('/home/seara/Desktop/Colab Notebooks/homework/hw06-text/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5507899",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(\"\", inplace=True)\n",
    "test.fillna(\"\", inplace=True)\n",
    "# Заполнение пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"desctitletokens\"] = test[\"title\"] + \" \" + test[\"description\"]\n",
    "train[\"desctitletokens\"] = train[\"title\"] + \" \" + train[\"description\"]\n",
    "test.drop([\"title\",\"description\"], axis=1, inplace=True)\n",
    "train.drop([\"title\",\"description\"], axis=1, inplace=True)\n",
    "# Создание единой колонки и дроп лишних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train[\"desctitletokens\"] = train[\"desctitletokens\"].apply(MyTokenizer)\n",
    "test[\"desctitletokens\"] = test[\"desctitletokens\"].apply(MyTokenizer)\n",
    "# Применение токенизатора ко всему датасету, чтобы потом можно было сразу токенизированный датафрейм подгружать из pickle файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('traintokenized', 'wb') as data:\n",
    "    pickle.dump(train, data)\n",
    "\n",
    "with open('testtokenized', 'wb') as data:\n",
    "    pickle.dump(test, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58777fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open(\n",
    "    \"traintokenized\", \"rb\"\n",
    ") as data:\n",
    "    train = pickle.load(data)\n",
    "with open(\"testtokenized\", \"rb\") as data:\n",
    "    test = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beauty(row):\n",
    "    text = ''\n",
    "    if row[\"Category\"] == 86:\n",
    "        if 'коммерческий' in row[\"desctitletokens\"].split(' '):\n",
    "            text = \" \".join(row[\"desctitletokens\"].split(' ')[0:3])\n",
    "        else:\n",
    "            text = \" \".join(row[\"desctitletokens\"].split(' ')[0:2])\n",
    "    else:\n",
    "        text = row[\"desctitletokens\"]\n",
    "    return text\n",
    "# Чуть изменим текст 86 категории в тренировочном датасете для лучших результатов\n",
    "# Не нашел способов как-то вручную улучшить тренировочные данные для остальных категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f97943",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train[\"desctitletokens\"] = train.apply(beauty, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa863083",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[\"desctitletokens\"]\n",
    "y = train[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce37de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = Pipeline([(\"vec\", TfidfVectorizer(ngram_range=(1, 2))), (\"clf\", SGDClassifier(n_jobs=-1, alpha=0.0000002,tol=1e-4))])\n",
    "clf.fit(X,y)\n",
    "# Обучаем на всей выборке\n",
    "# Параметры для SGD были подобраны с помощью GRIDSEARCH и многочисленных тестов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f305d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(test['desctitletokens'])\n",
    "data = {'Id':test[\"itemid\"],\n",
    "        'Category':pred}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"finalattempt.csv\",index=False)\n",
    "# Загружаем на kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e75792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
